{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# Reduce TensorFlow log noise\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "\n",
    "# Reset TensorFlow internal state\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# List available GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        print(f\"Found GPU: {gpu}\")\n",
    "        # Enable memory growth\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "else:\n",
    "    print(\"No GPUs found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and training parameters\n",
    "epochs = 1\n",
    "dataset = 10\n",
    "how = \"normal\"\n",
    "action = \"eval\"\n",
    "threshold = 0.5\n",
    "contrast = 1\n",
    "weight = 7.0\n",
    "distort = False\n",
    "batch_size = 32\n",
    "\n",
    "# Hyperparameters\n",
    "epsilon = 1e-8\n",
    "epochs_per_decay = 5\n",
    "decay_factor = 0.80\n",
    "staircase = True\n",
    "lamC = 0.00001\n",
    "lamF = 0.00250\n",
    "\n",
    "# Training options\n",
    "dropout = True\n",
    "fc_dropout_rate = 0.5\n",
    "conv_dropout_rate = 0.001\n",
    "pool_dropout_rate = 0.1\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_cell_guid": "5bde04c0-d9b9-4ca4-9b78-0724f4520da2",
    "_uuid": "2a9eec106f03717c5e807ade1ccf7c65f7ec0ab1"
   },
   "outputs": [],
   "source": [
    "def get_batches(X, y, batch_size, filenames=None, distort=False):\n",
    "    shuffled_idx = np.arange(len(y))\n",
    "    np.random.shuffle(shuffled_idx)\n",
    "    i, h, w, c = X.shape\n",
    "\n",
    "    for i in range(0, len(y), batch_size):\n",
    "        batch_idx = shuffled_idx[i:i + batch_size]\n",
    "        X_return = X[batch_idx]\n",
    "\n",
    "        if distort:\n",
    "            coin = np.random.binomial(1, 0.5, size=None)\n",
    "            if coin:\n",
    "                X_return = X_return[..., ::-1, :]\n",
    "\n",
    "        if filenames is None:\n",
    "            yield X_return, y[batch_idx]\n",
    "        else:\n",
    "            yield X_return, y[batch_idx], filenames[batch_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _scale_input_data(X, contrast=None, mu=104.1353, scale=255.0):\n",
    "    if contrast and contrast != 1.0:\n",
    "        X_adj = tf.image.adjust_contrast(X, contrast)\n",
    "    else:\n",
    "        X_adj = X\n",
    "\n",
    "    X_adj = tf.cast(X_adj, dtype=tf.float32)\n",
    "    X_adj = tf.subtract(X_adj, mu)\n",
    "    X_adj = tf.divide(X_adj, scale)\n",
    "    return X_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(images, labels, horizontal_flip=False, vertical_flip=False, augment_labels=False, mixup=0):\n",
    "    images = tf.cast(images, tf.float32)\n",
    "    \n",
    "    if horizontal_flip:\n",
    "        images = tf.image.random_flip_left_right(images)\n",
    "    if vertical_flip:\n",
    "        images = tf.image.random_flip_up_down(images)\n",
    "        \n",
    "    if mixup > 0:\n",
    "        beta = tf.random.beta([tf.shape(images)[0]], mixup, mixup)\n",
    "        ll = tf.reshape(beta, [-1, 1, 1, 1])\n",
    "        shuffled_images = tf.roll(images, shift=1, axis=0)\n",
    "        images = ll * images + (1 - ll) * shuffled_images\n",
    "        \n",
    "        if augment_labels:\n",
    "            labels = beta * labels + (1 - beta) * tf.roll(labels, shift=1, axis=0)\n",
    "            \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_cell_guid": "373b2cd7-8d34-4d3c-9b18-cc734cda0bee",
    "_uuid": "6d8f75507265b7ea89cc397002fc261f539edfba"
   },
   "outputs": [],
   "source": [
    "class MammographyCNN(tf.keras.Model):\n",
    "    def __init__(self, num_classes=2, dropout=True, fc_dropout_rate=0.5, \n",
    "                 conv_dropout_rate=0.001, pool_dropout_rate=0.1):\n",
    "        super(MammographyCNN, self).__init__()\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        self.fc_dropout_rate = fc_dropout_rate\n",
    "        self.conv_dropout_rate = conv_dropout_rate\n",
    "        self.pool_dropout_rate = pool_dropout_rate\n",
    "        \n",
    "        # Conv1 block\n",
    "        self.conv1 = tf.keras.layers.Conv2D(32, (3, 3), strides=(2, 2), padding='same',\n",
    "                                          kernel_regularizer=tf.keras.regularizers.l2(lamC))\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv11 = tf.keras.layers.Conv2D(32, (3, 3), padding='same',\n",
    "                                           kernel_regularizer=tf.keras.regularizers.l2(lamC))\n",
    "        self.bn11 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv12 = tf.keras.layers.Conv2D(32, (3, 3), padding='same',\n",
    "                                           kernel_regularizer=tf.keras.regularizers.l2(lamC))\n",
    "        self.bn12 = tf.keras.layers.BatchNormalization()\n",
    "        self.pool1 = tf.keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same')\n",
    "        \n",
    "        # Conv2 block\n",
    "        self.conv21 = tf.keras.layers.Conv2D(64, (3, 3), padding='same',\n",
    "                                           kernel_regularizer=tf.keras.regularizers.l2(lamC))\n",
    "        self.bn21 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv22 = tf.keras.layers.Conv2D(64, (3, 3), padding='same',\n",
    "                                           kernel_regularizer=tf.keras.regularizers.l2(lamC))\n",
    "        self.bn22 = tf.keras.layers.BatchNormalization()\n",
    "        self.pool2 = tf.keras.layers.MaxPooling2D((2, 2), padding='same')\n",
    "        \n",
    "        # Global Average Pooling to handle spatial dimensions\n",
    "        self.global_pool = tf.keras.layers.GlobalAveragePooling2D()\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = tf.keras.layers.Dense(2048, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(2048, activation='relu')\n",
    "        self.fc3 = tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = inputs  # Use inputs directly\n",
    "        \n",
    "        # Conv1 block\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        x = self.conv11(x)\n",
    "        x = self.bn11(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        x = self.conv12(x)\n",
    "        x = self.bn12(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        x = self.pool1(x)\n",
    "        if self.dropout and training:\n",
    "            x = tf.nn.dropout(x, rate=self.pool_dropout_rate)\n",
    "        \n",
    "        # Conv2 block\n",
    "        x = self.conv21(x)\n",
    "        x = self.bn21(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        x = self.conv22(x)\n",
    "        x = self.bn22(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        x = self.pool2(x)\n",
    "        if self.dropout and training:\n",
    "            x = tf.nn.dropout(x, rate=self.pool_dropout_rate)\n",
    "\n",
    "        # Global Average Pooling\n",
    "        x = self.global_pool(x)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def __init__(self, num_classes=2, dropout=True, fc_dropout_rate=0.5, \n",
    "                 conv_dropout_rate=0.001, pool_dropout_rate=0.1):\n",
    "        super(MammographyCNN, self).__init__()\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        self.fc_dropout_rate = fc_dropout_rate\n",
    "        self.conv_dropout_rate = conv_dropout_rate\n",
    "        self.pool_dropout_rate = pool_dropout_rate\n",
    "        \n",
    "        # Conv1 block\n",
    "        self.conv1 = tf.keras.layers.Conv2D(32, (3, 3), strides=(2, 2), padding='same',\n",
    "                                          kernel_regularizer=tf.keras.regularizers.l2(lamC))\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv11 = tf.keras.layers.Conv2D(32, (3, 3), padding='same',\n",
    "                                           kernel_regularizer=tf.keras.regularizers.l2(lamC))\n",
    "        self.bn11 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv12 = tf.keras.layers.Conv2D(32, (3, 3), padding='same',\n",
    "                                           kernel_regularizer=tf.keras.regularizers.l2(lamC))\n",
    "        self.bn12 = tf.keras.layers.BatchNormalization()\n",
    "        self.pool1 = tf.keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same')\n",
    "        \n",
    "        # Conv2 block\n",
    "        self.conv21 = tf.keras.layers.Conv2D(64, (3, 3), padding='same',\n",
    "                                           kernel_regularizer=tf.keras.regularizers.l2(lamC))\n",
    "        self.bn21 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv22 = tf.keras.layers.Conv2D(64, (3, 3), padding='same',\n",
    "                                           kernel_regularizer=tf.keras.regularizers.l2(lamC))\n",
    "        self.bn22 = tf.keras.layers.BatchNormalization()\n",
    "        self.pool2 = tf.keras.layers.MaxPooling2D((2, 2), padding='same')\n",
    "        \n",
    "        # Additional blocks as per original architecture...\n",
    "        # FC layers\n",
    "        self.fc1 = tf.keras.layers.Conv2D(2048, (5, 5), strides=(5, 5), padding='valid')\n",
    "        self.bn_fc1 = tf.keras.layers.BatchNormalization()\n",
    "        self.fc2 = tf.keras.layers.Conv2D(2048, (1, 1), padding='valid')\n",
    "        self.bn_fc2 = tf.keras.layers.BatchNormalization()\n",
    "        self.fc3 = tf.keras.layers.Dense(num_classes)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = inputs  # Use inputs directly\n",
    "        \n",
    "        # Conv1 block\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        x = self.conv11(x)\n",
    "        x = self.bn11(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        x = self.conv12(x)\n",
    "        x = self.bn12(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        x = self.pool1(x)\n",
    "        if self.dropout and training:\n",
    "            x = tf.nn.dropout(x, rate=self.pool_dropout_rate)\n",
    "        \n",
    "        # Conv2 block\n",
    "        x = self.conv21(x)\n",
    "        x = self.bn21(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        x = self.conv22(x)\n",
    "        x = self.bn22(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        x = self.pool2(x)\n",
    "        if self.dropout and training:\n",
    "            x = tf.nn.dropout(x, rate=self.pool_dropout_rate)\n",
    "            \n",
    "        # FC layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn_fc1(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn_fc2(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        x = tf.squeeze(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "_cell_guid": "063e9183-960a-4a96-bc11-246b84720d56",
    "_uuid": "91044dd38f433b93fa5dc5cafca87059fe8e3d6d"
   },
   "outputs": [],
   "source": [
    "def get_training_data(what=10):\n",
    "    if what == 10:\n",
    "        train_path_10 = \"data/training10_0.tfrecords\"\n",
    "        train_path_11 = \"data/training10_1.tfrecords\"\n",
    "        train_path_12 = \"data/training10_2.tfrecords\"\n",
    "        train_path_13 = \"data/training10_3.tfrecords\"\n",
    "        \n",
    "        train_files = [train_path_10, train_path_11, train_path_12, train_path_13]\n",
    "        total_records = 44712\n",
    "    return train_files, total_records\n",
    "\n",
    "def get_test_data(what=10):\n",
    "    test_files = \"data/training10_4.tfrecords\"\n",
    "    return [test_files], 11178"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_cell_guid": "323fc88f-f299-467c-88ca-f374051c9e1f",
    "_uuid": "1a1b5f48bd09ae3dbbb30dfbd2c6769967392bb5"
   },
   "outputs": [],
   "source": [
    "def create_dataset(filenames, batch_size, is_training=False):\n",
    "    feature_description = {\n",
    "        'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'label_normal': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image': tf.io.FixedLenFeature([], tf.string)\n",
    "    }\n",
    "    \n",
    "    def _parse_function(example_proto):\n",
    "        parsed_features = tf.io.parse_single_example(example_proto, feature_description)\n",
    "        image = tf.io.decode_raw(parsed_features['image'], tf.uint8)\n",
    "        image = tf.reshape(image, [299, 299, 1])\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        return image, parsed_features['label_normal']\n",
    "    \n",
    "    dataset = tf.data.TFRecordDataset(filenames)\n",
    "    dataset = dataset.map(_parse_function)\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(buffer_size=1000)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_cell_guid": "beda6f27-f3da-470a-9eee-268bf40d9a7e",
    "_uuid": "a210632ca0c39d20103cc5d92354ca16e255d5d8"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(model, images, labels, optimizer, loss_fn):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images, training=True)\n",
    "        weights = tf.multiply(weight, tf.cast(tf.greater(labels, 0), tf.float32)) + 1\n",
    "        loss = loss_fn(labels, predictions, sample_weight=weights)\n",
    "        \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_uuid": "65642bdf7b7b029daf76501f0f8e4106e9fc25db"
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "train_files, total_records = get_training_data(dataset)\n",
    "test_files, test_records = get_test_data(dataset)\n",
    "steps_per_epoch = int(total_records / batch_size)\n",
    "\n",
    "# Create model and optimizer\n",
    "model = MammographyCNN(num_classes=num_classes, dropout=dropout,\n",
    "                      fc_dropout_rate=fc_dropout_rate,\n",
    "                      conv_dropout_rate=conv_dropout_rate,\n",
    "                      pool_dropout_rate=pool_dropout_rate)\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    0.001, decay_steps=steps_per_epoch*epochs_per_decay,\n",
    "    decay_rate=decay_factor, staircase=staircase)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Metrics\n",
    "train_loss = tf.keras.metrics.Mean('train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('train_accuracy')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('test_accuracy')\n",
    "test_precision = tf.keras.metrics.Precision()\n",
    "test_recall = tf.keras.metrics.Recall()\n",
    "\n",
    "# Checkpointing\n",
    "checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)\n",
    "checkpoint_manager = tf.train.CheckpointManager(checkpoint, './model', max_to_keep=3)\n",
    "\n",
    "# Load checkpoints if they exist\n",
    "if os.path.exists('./model'):\n",
    "    print(\"Restoring from checkpoint...\")\n",
    "    checkpoint.restore(checkpoint_manager.latest_checkpoint)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = create_dataset(train_files, batch_size, is_training=True)\n",
    "test_dataset = create_dataset(test_files, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"data/training10_4.tfrecords\"\n",
    "print(f\"File exists: {os.path.exists(test_path)}\")\n",
    "\n",
    "# List contents of /data\n",
    "print(\"\\nContents of /data:\")\n",
    "print(os.listdir('data/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fd60a388-f2d2-45e8-9b60-b8d6d4ca752d",
    "_uuid": "acb830fab334c5e4fc0159cd539ecb2995f1280c"
   },
   "outputs": [],
   "source": [
    "# Training/evaluation loop\n",
    "checkpoint_every = 5  # Define checkpoint interval\n",
    "print(model.summary())\n",
    "\n",
    "# Training/evaluation loop\n",
    "if action == \"train\":\n",
    "    print(\"Training model...\")\n",
    "    for epoch in range(epochs):\n",
    "        train_loss.reset_state()  # Correct method\n",
    "        train_accuracy.reset_state()  # Correct method\n",
    "\n",
    "        for images, labels in train_dataset:\n",
    "            # Scale input images if required\n",
    "            scaled_images = _scale_input_data(images)\n",
    "\n",
    "            # Perform a training step\n",
    "            loss, predictions = train_step(model, scaled_images, labels, optimizer, loss_fn)\n",
    "\n",
    "            # Convert predictions to class indices if using sparse labels\n",
    "            predictions = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "            # Debugging: Print shapes\n",
    "            print(f\"Train Images Shape: {scaled_images.shape}, Labels Shape: {labels.shape}, Predictions Shape: {predictions.shape}\")\n",
    "\n",
    "            # Update metrics\n",
    "            train_loss.update_state(loss)\n",
    "            train_accuracy.update_state(labels, predictions)\n",
    "\n",
    "        # Save model checkpoint every few epochs\n",
    "        if epoch % checkpoint_every == 0:\n",
    "            checkpoint_manager.save()\n",
    "\n",
    "        # Print epoch summary\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {train_loss.result()}, Accuracy: {train_accuracy.result()}\")\n",
    "else:  # Evaluation\n",
    "    print(\"Evaluating model...\")\n",
    "    test_accuracy.reset_state()\n",
    "    test_precision.reset_state()\n",
    "    test_recall.reset_state()\n",
    "\n",
    "    for test_images, test_labels in test_dataset:\n",
    "        # Scale input images\n",
    "        scaled_images = _scale_input_data(test_images)\n",
    "\n",
    "        # Get predictions (now in [batch_size, num_classes] shape)\n",
    "        predictions = model(scaled_images, training=False)\n",
    "\n",
    "        # Debugging: Print shapes and examples\n",
    "        print(f\"Test Images Shape: {scaled_images.shape}\")\n",
    "        print(f\"Test Labels Shape: {test_labels.shape}, Predictions Shape: {predictions.shape}\")\n",
    "\n",
    "        # Update metrics (no need to apply tf.argmax for SparseCategoricalAccuracy)\n",
    "        test_accuracy.update_state(test_labels, predictions)\n",
    "        test_precision.update_state(test_labels, tf.argmax(predictions, axis=-1))  # Precision needs class indices\n",
    "        test_recall.update_state(test_labels, tf.argmax(predictions, axis=-1))  # Recall needs class indices\n",
    "\n",
    "\n",
    "\n",
    "    # Print evaluation results\n",
    "    print(f\"Test Accuracy: {test_accuracy.result().numpy():.4f}\")\n",
    "    print(f\"Test Precision: {test_precision.result().numpy():.4f}\")\n",
    "    print(f\"Test Recall: {test_recall.result().numpy():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 27228,
     "sourceId": 34719,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 27069,
     "sourceId": 46974,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 1599,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
